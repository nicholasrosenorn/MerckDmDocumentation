[
["creation-of-dummy-data.html", "Chapter 5 Creation of Dummy Data 5.1 Why do we need dummy data? 5.2 How we generated dummy data? 5.3 How to run the scripts? 5.4 Potential improvements", " Chapter 5 Creation of Dummy Data 5.1 Why do we need dummy data? In order to effectively test the functionality of the backend code, a lot of pre-existing data is needed to fill the database to simulate a normal functioning environment for the app. Initially, we only had about 40 users’ data, and it was not enough. Therefore, we decided to create fake user data based on pre-existing user data (dummy data). 5.2 How we generated dummy data? In order to let dummy data make sense and follow pre-existing data’s patterns, we studied the patterns of each pre-existing data category. There are active, basal, dist, elev, heart, and steps data. We mostly used user 185 and 186’s dataset as the reference while creating dummy dataset. For the active data, we first calculated the mean value of the original data. Then we found out the number of data points above the mean. Because it can be observed that within each active data set, data points above the mean value cluster into three group, and one of them always starts at the beginning. Therefore, we also chose the start as the position of the first high value data cluster, and randomly chose position for the other two. The value of each data point was chosen based on the observed range of the reference data point. The result of fake data vs. real data graph can be found below: For other data categories, we utilized similar approaches while ajusting parameters and the possible range of random number generation based on each data categories’ own characteristics. Below are the fake data vs. real data comparison graphs for other data categories. For basal data: For Dist data: For Elev data: For Heart data: For Steps data: 5.3 How to run the scripts? In order to run the scripts, some modifications to the GenerateFakeData.py are needed. There are three parameters that need modification: folder_name_list, real_name_list, and path. “folder_name_list” tells the name of the folders that will contain the dummy data. “real_name_list” tells the name of the reference files being used to generate dummy data. “path” is the current directory. Once these three parameters are set up correctly, just directly run the GenerateFakeData.py script with no input. 5.4 Potential improvements Because our dummy data generation script is specifically modified for each data set, if in the future a new type of data appears, we need to rewrite our code in order to accommodate the new data type. Therefore, we can improve our code by applying more statictical knowledge so that the code can be applied to generate dummy data from more different data types. "],
["creating-a-connection-between-api-endpoints.html", "Chapter 6 Creating a connection between API Endpoints 6.1 List of API end-points: 6.2 What framework we are going to use? 6.3 Setting up SQL database connection: 6.4 Design of the Routes", " Chapter 6 Creating a connection between API Endpoints 6.1 List of API end-points: Front-End Patient User input to the database Biometric Data from the FitBit API to the database Data from the database to the RShiny Dashboard for visualization 6.2 What framework we are going to use? We are currently looking to use the framework Express.js to write the routes between the database and various API endpoints. Express is a fast, light-weight web framework for Node.js. Express is a pretty good framework. It’s the most popular node application framework out there. Express is widely used as middleware in node apps as it helps to organize your app into a MVC architecture. It’s the E of the popular MEAN stack. Express manages following things very easily: Routing Sessions HTTP requests Error handling At times writing code from scratch for above things can be time consuming. But by using express it’s only a matter of few methods. Express also helps in organizing your code. Link to set up Node 6.3 Setting up SQL database connection: We created a connection with the MariaDB database in our Node Rest API server to be able to send and receive data. The following link shows a tutorial on how we set up the connection. Link to setup SQL connection in Node 6.4 Design of the Routes There are five kinds of routes: GET: The GET method requests a representation of the specified resource. Requests using GET should only retrieve data and should have no other effect. POST: The POST method requests that the server accept the entity enclosed in the request as a new subordinate of the web resource identified by the URI. The data POSTed might be, for example, an annotation for existing resources; a message for a bulletin board, newsgroup, mailing list, or comment thread; a block of data that is the result of submitting a web form to a data-handling process; or an item to add to a database. PUT: The PUT method requests that the enclosed entity be stored under the supplied URI. If the URI refers to an already existing resource, it is modified; if the URI does not point to an existing resource, then the server can create the resource with that URI. DELETE: The DELETE method deletes the specified resource. PATCH: The PATCH method applies partial modifications to a resource "],
["hosting-the-database-on-a-server.html", "Chapter 7 Hosting the database on a Server 7.1 Issues faced with hosting a database on Scholar: 7.2 Work around 7.3 How we proceeded", " Chapter 7 Hosting the database on a Server 7.1 Issues faced with hosting a database on Scholar: Scholar is a small computer cluster, suitable for classroom learning about high performance computing (HPC). It can be accessed as a typical cluster, with a job scheduler distributing batch jobs onto its worker nodes, or as an interactive resource, with software packages available through a desktop-like environment on its login servers. We tried to create a connection to the sql server hosted on this server from our local but we faced issues because there was a firewall preventing access to the database from a foreign server We tried running our Backend API on scholar but we were unable to install NodeJS, MySQLWorkbench and other packages on the server without authorization. 7.2 Work around In order to install packages on scholar, we are expected to make requests to the administration with a list of all program lines to run in the form of a SLURM job. The Simple Linux Utility for Resource Management (SLURM) is a system providing job scheduling and job management on compute clusters. With SLURM, a user requests resources and submits a job to a queue. The system will then take jobs from queues, allocate the necessary nodes, and execute them. To submit work to a SLURM queue, you must first create a job submission file. More info can be found at the following link: SLURM Job 7.3 How we proceeded Since we have to make a request each time we have to install a package, we decided to make just one request with a complete list of all installation. As a result, we wanted to host a temporary database on AWS that we can connect to and test on using our local machine. We created a copy of the entire database on AWS with the following credentials: Hostname: rnr56s6e2uk326pj.cbetxkdyhwsb.us-east-1.rds.amazonaws.com Username: cb3i17t0aqn6a4ff Password: e2l4k9zn24shcj42 "],
["adding-data-to-the-database.html", "Chapter 8 Adding data to the database 8.1 Adding a CSV to the database 8.2 POST information 8.3 Patient information and Study Specific Data", " Chapter 8 Adding data to the database 8.1 Adding a CSV to the database The data engineering team had made a script that creates a CSV file with all the users FitBit data when they make a request to the API We decided to make a python script to load this csv data onto the database we hosted on AWS. 8.2 POST information The JSON to add a datapoint to the database is as follows: obj = {&#39;collection_date&#39;: row[&quot;Date&quot;], &#39;steps&#39;: row[&quot;Steps&quot;], &#39;floors_climbed&#39;: row[&quot;Floors Climbed&quot;], &#39;total_miles&#39;: row[&quot;Total Miles&quot;], &#39;lightly_active_miles&#39;: [&quot;Lightly Active Miles&quot;], &#39;moderately_active_miles&#39;: row[&quot;Moderately Active Miles&quot;], &#39;very_active_miles&#39;: row[&quot;Very Active Miles&quot;], &#39;sedentary_minutes&#39;: row[&quot;Sedentary Minutes&quot;], &#39;lightly_active_minutes&#39;: row[&quot;Lightly Active Minutes&quot;], &#39;fairly_active_minutes&#39;: row[&quot;Fairly Active Minutes&quot;], &#39;very_active_minutes&#39;: row[&quot;Very Active Minutes&quot;], &#39;hr30_100_minutes&#39;: row[&quot;HR 30-100 Minutes&quot;], &#39;hr100_140_minutes&#39;: row[&quot;HR 100-140 Minutes&quot;], &#39;hr140_170_minutes&#39;: row[&quot;HR 140-170 Minutes&quot;], &#39;hr170_220_minutes&#39;: row[&quot;HR 170-220 Minutes&quot;], &#39;average_resting_heartrate&#39;: row[&quot;Average Resting HR&quot;], &#39;bmi&#39;: row[&quot;BMI&quot;], &#39;sleep_efficiency&#39;: row[&quot;Sleep Efficiency&quot;], &#39;weight&#39;: row[&quot;Weight&quot;], &#39;minutes_asleep&#39;: row[&quot;Minutes Alseep&quot;], &#39;fbusername&#39;: row[&quot;username&quot;] } We made structures in this form by reading the CSV information into a pandas dataframe and made a post request to our API 8.3 Patient information and Study Specific Data The patient information is sent to the database form a webpage which will be used by Merck Scientists to load the patients they are studying during a clinical trial. The JSON for a patient is as follows: { patient_id, fbusername, first_name, last_name, gender, date_of_birth, height } The JSON for the study data is as follows: { patient_id, input_date, family_history_cancer, family_history_heart_disease, diagnostic_notes } "]
]
